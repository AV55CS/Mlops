{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0f482c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear some memory before loading new data\n",
    "import gc\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ed295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /home/codespace/anaconda3/lib/python3.9/site-packages (20.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a00b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "566bd666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a18b4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read  data\n",
    "\n",
    "df=pd.read_parquet(\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\")\n",
    "\n",
    "df_feb = pd.read_parquet(\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a6790a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
       "       'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
       "       'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',\n",
       "       'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
       "       'total_amount', 'congestion_surcharge', 'airport_fee'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79bcadc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c30ba3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VendorID                          int64\n",
      "tpep_pickup_datetime     datetime64[ns]\n",
      "tpep_dropoff_datetime    datetime64[ns]\n",
      "passenger_count                 float64\n",
      "trip_distance                   float64\n",
      "RatecodeID                      float64\n",
      "store_and_fwd_flag               object\n",
      "PULocationID                      int64\n",
      "DOLocationID                      int64\n",
      "payment_type                      int64\n",
      "fare_amount                     float64\n",
      "extra                           float64\n",
      "mta_tax                         float64\n",
      "tip_amount                      float64\n",
      "tolls_amount                    float64\n",
      "improvement_surcharge           float64\n",
      "total_amount                    float64\n",
      "congestion_surcharge            float64\n",
      "airport_fee                     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check data types of the DataFrame\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f3b723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of trip duration: 42.59\n"
     ]
    }
   ],
   "source": [
    "# # Convert datetime columns to datetime type if needed\n",
    "# df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "# df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "\n",
    "# Calculate duration in minutes\n",
    "df['duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "# Calculate standard deviation\n",
    "duration_std = df['duration'].std()\n",
    "print(f\"Standard deviation of trip duration: {duration_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0345f944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3066766\n",
      "57593\n",
      "Fraction of records remaining: 98.12%\n"
     ]
    }
   ],
   "source": [
    "# Count total records before filtering\n",
    "total_records = len(df)\n",
    "print(total_records)\n",
    "\n",
    "# Filter to keep only trips between 1 and 60 minutes\n",
    "df_filtered = df[(df['duration'] >= 1) & (df['duration'] <= 60)]\n",
    "\n",
    "# Count remaining records and calculate percentage\n",
    "remaining_records = len(df_filtered)\n",
    "filtered_record=total_records-remaining_records\n",
    "print(filtered_record)\n",
    "fraction_remaining = remaining_records / total_records\n",
    "print(f\"Fraction of records remaining: {fraction_remaining:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "510b202a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of feature matrix: 515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Select only the location ID columns and convert to strings\n",
    "df_loc = df_filtered[['PULocationID', 'DOLocationID']].copy()\n",
    "df_loc['PULocationID'] = df_loc['PULocationID'].astype(str)\n",
    "df_loc['DOLocationID'] = df_loc['DOLocationID'].astype(str)\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "dict_list = df_loc.to_dict(orient='records')\n",
    "\n",
    "# Fit a dictionary vectorizer\n",
    "dv = DictVectorizer()\n",
    "X = dv.fit_transform(dict_list)\n",
    "\n",
    "# Print dimensionality (number of columns)\n",
    "print(f\"Dimensionality of feature matrix: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0d4ae0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear some memory before loading new data\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42a98c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training data: 7.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Prepare target variable\n",
    "y = df_filtered['duration'].values\n",
    "\n",
    "# Train linear regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Make predictions and calculate RMSE\n",
    "y_pred = lr.predict(X)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f\"RMSE on training data: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7398fd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear some memory before loading new data\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6e33d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded February data with 2913955 records\n",
      "After filtering: 2855951 records\n",
      "Processing batch 1/29\n",
      "Processing batch 2/29\n",
      "Processing batch 3/29\n",
      "Processing batch 4/29\n",
      "Processing batch 5/29\n",
      "Processing batch 6/29\n",
      "Processing batch 7/29\n",
      "Processing batch 8/29\n",
      "Processing batch 9/29\n",
      "Processing batch 10/29\n",
      "Processing batch 11/29\n",
      "Processing batch 12/29\n",
      "Processing batch 13/29\n",
      "Processing batch 14/29\n",
      "Processing batch 15/29\n",
      "Processing batch 16/29\n",
      "Processing batch 17/29\n",
      "Processing batch 18/29\n",
      "Processing batch 19/29\n",
      "Processing batch 20/29\n",
      "Processing batch 21/29\n",
      "Processing batch 22/29\n",
      "Processing batch 23/29\n",
      "Processing batch 24/29\n",
      "Processing batch 25/29\n",
      "Processing batch 26/29\n",
      "Processing batch 27/29\n",
      "Processing batch 28/29\n",
      "Processing batch 29/29\n",
      "RMSE on validation data: 7.81\n"
     ]
    }
   ],
   "source": [
    "# Clear memory\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Import necessary libraries \n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Function to process February data in a memory-efficient way\n",
    "def process_february_data(dv, model):\n",
    "    # Path to February data\n",
    "    feb_path = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\"\n",
    "    \n",
    "    # Read only necessary columns to save memory\n",
    "    df_feb = pd.read_parquet(\n",
    "        feb_path, \n",
    "        columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID']\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded February data with {len(df_feb)} records\")\n",
    "    \n",
    "    # Calculate duration\n",
    "    df_feb['duration'] = (\n",
    "        pd.to_datetime(df_feb['tpep_dropoff_datetime']) - \n",
    "        pd.to_datetime(df_feb['tpep_pickup_datetime'])\n",
    "    ).dt.total_seconds() / 60\n",
    "    \n",
    "    # Filter outliers\n",
    "    df_feb = df_feb[(df_feb['duration'] >= 1) & (df_feb['duration'] <= 60)]\n",
    "    print(f\"After filtering: {len(df_feb)} records\")\n",
    "    \n",
    "    # Free memory\n",
    "    df_feb.drop(['tpep_pickup_datetime', 'tpep_dropoff_datetime'], axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Process in smaller batches to avoid memory issues\n",
    "    batch_size = 100000\n",
    "    n_batches = (len(df_feb) // batch_size) + 1\n",
    "    \n",
    "    sum_squared_errors = 0\n",
    "    total_records = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df_feb))\n",
    "        \n",
    "        if start_idx >= len(df_feb):\n",
    "            break\n",
    "            \n",
    "        print(f\"Processing batch {i+1}/{n_batches}\")\n",
    "        \n",
    "        # Get batch\n",
    "        batch = df_feb.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Prepare features\n",
    "        batch['PULocationID'] = batch['PULocationID'].astype(str)\n",
    "        batch['DOLocationID'] = batch['DOLocationID'].astype(str)\n",
    "        dict_list = batch[['PULocationID', 'DOLocationID']].to_dict(orient='records')\n",
    "        \n",
    "        # Transform and predict\n",
    "        X = dv.transform(dict_list)\n",
    "        y = batch['duration'].values\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Update running sum of squared errors\n",
    "        sum_squared_errors += np.sum((y - y_pred) ** 2)\n",
    "        total_records += len(y)\n",
    "        \n",
    "        # Clear memory\n",
    "        del batch, dict_list, X, y, y_pred\n",
    "        gc.collect()\n",
    "    \n",
    "    # Calculate final RMSE\n",
    "    rmse = np.sqrt(sum_squared_errors / total_records)\n",
    "    return rmse\n",
    "\n",
    "# Call the function with your trained model and DictVectorizer\n",
    "rmse_feb = process_february_data(dv, lr)\n",
    "print(f\"RMSE on validation data: {rmse_feb:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
